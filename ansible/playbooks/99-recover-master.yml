---
- name: Recover from Failed Kubernetes Master Initialization
  hosts: masters[0]
  become: true
  vars_files:
    - ../group_vars/all.yml
  vars:
    ansible_ssh_common_args: '-o ProxyCommand="ssh -W %h:%p -q ec2-user@{{ hostvars["bastion-host"]["ansible_host"] }} -i /home/ec2-user/.ssh/id_rsa -o StrictHostKeyChecking=no"'
  tasks:
    - name: Check if admin.conf exists (cluster partially initialized)
      stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf

    - name: Create .kube directory for ec2-user
      file:
        path: /home/ec2-user/.kube
        state: directory
        owner: ec2-user
        group: ec2-user
        mode: '0755'
      when: admin_conf.stat.exists

    - name: Copy admin.conf to ec2-user kube config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/ec2-user/.kube/config
        remote_src: true
        owner: ec2-user
        group: ec2-user
        mode: '0644'
      when: admin_conf.stat.exists

    - name: Create .kube directory for root
      file:
        path: /root/.kube
        state: directory
        mode: '0755'
      when: admin_conf.stat.exists

    - name: Copy admin.conf to root kube config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: true
        mode: '0644'
      when: admin_conf.stat.exists

    - name: Wait for API server to be ready
      wait_for:
        host: "{{ ansible_default_ipv4.address }}"
        port: 6443
        timeout: 120
      when: admin_conf.stat.exists

    - name: Check if CoreDNS is running
      shell: kubectl get pods -n kube-system -l k8s-app=kube-dns --no-headers
      register: coredns_check
      become_user: ec2-user
      ignore_errors: true
      when: admin_conf.stat.exists

    - name: Apply CoreDNS manually if missing
      shell: kubectl apply -f https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed
      become_user: ec2-user
      when: admin_conf.stat.exists and (coredns_check.stdout == "" or coredns_check.rc != 0)
      ignore_errors: true

    - name: Generate join command for workers
      command: kubeadm token create --print-join-command
      register: join_command
      when: admin_conf.stat.exists

    - name: Save join command to local file
      local_action:
        module: copy
        content: "{{ join_command.stdout }}"
        dest: "../join-command.sh"
        mode: '0755'
      when: admin_conf.stat.exists and join_command is defined

    - name: Generate certificate key for control plane join
      command: kubeadm init phase upload-certs --upload-certs
      register: cert_key
      when: admin_conf.stat.exists

    - name: Generate join command for control plane
      shell: |
        echo "{{ join_command.stdout }} --control-plane --certificate-key {{ cert_key.stdout_lines[-1] }}"
      register: control_plane_join
      when: admin_conf.stat.exists and join_command is defined and cert_key is defined

    - name: Save control plane join command
      local_action:
        module: copy
        content: "{{ control_plane_join.stdout }}"
        dest: "../control-plane-join-command.sh"
        mode: '0755'
      when: admin_conf.stat.exists and control_plane_join is defined

    - name: Display recovery status
      debug:
        msg: |
          ðŸ”§ Recovery Status:
          - Admin config exists: {{ admin_conf.stat.exists }}
          - Join command generated: {{ join_command is defined and join_command.stdout is defined }}
          - Control plane join ready: {{ control_plane_join is defined and control_plane_join.stdout is defined }}
          
          Next steps:
          1. Run network setup playbook: ansible-playbook -i inventory/hosts.yml playbooks/05-network-setup.yml
          2. Run worker join playbook: ansible-playbook -i inventory/hosts.yml playbooks/04-workers-join.yml
          3. Run HA masters playbook: ansible-playbook -i inventory/hosts.yml playbooks/06-ha-masters.yml
