---
- name: Join Additional Master Nodes for High Availability
  hosts: masters[1:]  # All masters except the first one
  become: true
  vars:
    ansible_ssh_common_args: '-o ProxyCommand="ssh -W %h:%p -q ec2-user@{{ hostvars["bastion-host"]["ansible_host"] }} -i /home/ec2-user/.ssh/id_rsa -o StrictHostKeyChecking=no"'
  tasks:
    - name: Check if master is already joined
      stat:
        path: /etc/kubernetes/admin.conf
      register: admin_conf

    - name: Read control plane join command
      local_action:
        module: slurp
        src: "../control-plane-join-command.sh"
      register: cp_join_command
      when: not admin_conf.stat.exists
      ignore_errors: true

    - name: Fail if control plane join command not found
      fail:
        msg: "Control plane join command file not found. Run the master initialization playbook first."
      when: not admin_conf.stat.exists and (cp_join_command is failed or cp_join_command.content is not defined)

    - name: Join master to cluster
      shell: "{{ cp_join_command.content | b64decode | trim }}"
      when: not admin_conf.stat.exists and cp_join_command.content is defined
      register: cp_join_result
      retries: 3
      delay: 30

    - name: Wait for master join to complete
      pause:
        seconds: 30
      when: cp_join_result is changed

    - name: Create .kube directory for ec2-user
      file:
        path: /home/ec2-user/.kube
        state: directory
        owner: ec2-user
        group: ec2-user
        mode: '0755'

    - name: Copy admin.conf to ec2-user kube config
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/ec2-user/.kube/config
        remote_src: true
        owner: ec2-user
        group: ec2-user
        mode: '0644'

    - name: Set proper KUBECONFIG for ec2-user
      lineinfile:
        path: /home/ec2-user/.bashrc
        line: 'export KUBECONFIG=/home/ec2-user/.kube/config'
        create: yes
        owner: ec2-user
        group: ec2-user

    - name: Wait for master node to be ready
      pause:
        seconds: 90

    - name: Verify master node status
      shell: kubectl get nodes -l node-role.kubernetes.io/control-plane --no-headers --kubeconfig=/home/ec2-user/.kube/config | grep "{{ ansible_default_ipv4.address }}"
      register: master_status
      become_user: ec2-user

    - name: Display HA master status
      debug:
        msg: |
          âœ… Master node {{ inventory_hostname }} joined for HA!
          
          Master Status:
          {{ master_status.stdout }}
          
          ðŸ“Š High Availability Setup Complete!

- name: Verify High Availability Cluster
  hosts: masters[0]
  become: false
  vars:
    ansible_ssh_common_args: '-o ProxyCommand="ssh -W %h:%p -q ec2-user@{{ hostvars["bastion-host"]["ansible_host"] }} -i /home/ec2-user/.ssh/id_rsa -o StrictHostKeyChecking=no"'
  tasks:
    - name: Get all cluster nodes
      shell: kubectl get nodes -o wide --kubeconfig=/home/ec2-user/.kube/config
      register: all_nodes

    - name: Get control plane status
      shell: kubectl get pods -n kube-system --kubeconfig=/home/ec2-user/.kube/config | grep -E "(kube-apiserver|kube-controller-manager|kube-scheduler|etcd)"
      register: control_plane_pods

    - name: Display final cluster status
      debug:
        msg: |
          ðŸŽ‰ HIGH AVAILABILITY KUBERNETES CLUSTER READY! ðŸŽ‰
          
          ðŸ“Š All Cluster Nodes:
          {{ all_nodes.stdout }}
          
          ðŸ”§ Control Plane Components:
          {{ control_plane_pods.stdout }}
          
          âœ… Your multi-master Kubernetes cluster is now operational!
