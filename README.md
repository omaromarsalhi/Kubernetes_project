# Kubernetes Cluster Implementation
**Important:** IP addresses for all services (load balancer, masters, workers, etcd, apps) change every time you run `terraform apply`. 
**Always check and update `ansible/inventory.ini` with the latest IPs from `terraform output` before running any Ansible playbooks or accessing services.**
## Production-Read### Infrastructure Deployment
```bash
cd terraform
terraform init
terraform plan
terraform apply
```

### Update Inventory IPs After Terraform Changes
After running `terraform apply`, update the Ansible inventory with new IPs:

```bash
# Option 1: Automatic update (if Terraform outputs are configured)
cd terraform
chmod +x update-inventory.sh
./update-inventory.sh

# Option 2: Manual update - check outputs and edit ansible/inventory.ini
cd terraform
terraform output
```

**Note**: IP addresses change with each Terraform deployment. Always update `ansible/inventory.ini` with current IPs before running Ansible playbooks.netes Cluster on AWS

This project implements a production-ready Kubernetes cluster following the Medium guide: [Deploying a Production Kubernetes Cluster in 2023-2024](https://medium.com/@augustineozor/deploying-a-production-ready-kubernetes-cluster-in-2023-2024-a-step-by-step-guide-part-1-8f6b2e8c1c1a)

## ğŸ—ï¸ Architecture Overview

```
Internet â†’ Load Balancer (HAProxy) â†’ [ETCD Cluster | K3s Masters | Workers]
                                      â†“
                               Shared Storage (iSCSI + OCFS2)
```

### ğŸ“‹ Infrastructure Diagram
For a detailed visual representation of the infrastructure, see:
- **Interactive Diagram**: [infra_with_nfs.html](infra_with_nfs.html)
- **Static Image**: [kubernetes-infrastructure-nfs.png](kubernetes-infrastructure-nfs.png)

![Kubernetes Infrastructure with NFS](kubernetes-infrastructure-nfs.png)

## ğŸ“Š Current Status

### âœ… COMPLETED & DEPLOYED COMPONENTS
- **Infrastructure**: VPC, subnets, security groups, EC2 instances (Terraform) âœ…
- **HAProxy Load Balancer**: Configured for ETCD (2379), K8s API (6443), Dashboard (8443), Apps (30080/30081) âœ…
- **ETCD Cluster**: 2-node cluster with TLS encryption âœ…
- **K3s Kubernetes**: Masters and workers deployed with external etcd âœ…
- **Kubernetes Dashboard**: Installed and exposed via HAProxy âœ…
- **Security Groups**: Updated with proper access rules âœ…
- **Certificate Management**: TLS certificates generated and distributed âœ…
- **Sample Applications**: Simple web app and MongoDB + Mongo Express deployed âœ…

### ğŸ”„ IN PROGRESS
- **Storage**: iSCSI + OCFS2 shared storage pending
- **Monitoring**: Prometheus/Grafana stack
- **Backup**: ETCD and cluster backups

### ğŸ“ DEPLOYMENT STATUS
Deployment IPs (examples, update after every Terraform apply):
- **ETCD Nodes**: See `terraform output etcd1_private_ip` and `etcd2_private_ip`
- **HAProxy Load Balancer**: See `terraform output load_balancer_public_ip`
- **K3s Masters**: See `terraform output master1_private_ip` and `master2_private_ip`
- **K3s Workers**: See `terraform output worker1_private_ip` and `worker2_private_ip`
- **ETCD Endpoint**: `https://<LB_IP>:2379` (LB IP from output)
- **K8s API**: `https://<LB_IP>:6443`
- **HAProxy Stats**: `http://<LB_IP>:8399/stats_secure`

## ğŸš€ Quick Start

### Prerequisites
- AWS CLI configured
- Terraform v1.0+
- Ansible v2.9+
- SSH key pair
- kubectl (optional, for direct cluster access)

### 1. Infrastructure Deployment
```bash
cd terraform
terraform init
terraform plan
terraform apply
```

### 2. Full Cluster Deployment
```bash
cd ansible
ansible-playbook -i inventory.ini playbooks/deploy-all.yml
```

### 3. Deploy Sample Applications
```bash
# Simple web app
ansible-playbook -i inventory.ini playbooks/deploy-app.yml

# MongoDB + Mongo Express
ansible-playbook -i inventory.ini playbooks/deploy-mongo.yml
```

### Individual Component Deployment
```bash
# ETCD cluster
ansible-playbook -i inventory.ini playbooks/etcd-playbook.yml

# HAProxy load balancer
ansible-playbook -i inventory.ini playbooks/haproxy-playbook.yml

# K3s masters
ansible-playbook -i inventory.ini playbooks/k3s-masters.yml

# K3s workers
ansible-playbook -i inventory.ini playbooks/k3s-workers.yml

# Kubernetes Dashboard
ansible-playbook -i inventory.ini playbooks/k8s-dashboard.yml
```

## ğŸŒ Access Points
## ğŸŒ Access Points

**Always use the latest IPs from `terraform output` and update `ansible/inventory.ini` before accessing any service!**

| Service                | URL (replace <LB_IP> with current)         | Credentials         |
|------------------------|--------------------------------------------|---------------------|
| Kubernetes Dashboard   | `https://<LB_IP>:8443`                     | Token from deployment|
| Simple Web App         | `http://<LB_IP>:30080`                     | -                   |
| Mongo Express          | `http://<LB_IP>:30081`                     | admin/express123    |
| HAProxy Stats          | `http://<LB_IP>:8399/stats_secure`         | admin/omar123       |
| K8s API                | `https://<LB_IP>:6443`                     | -                   |
| ETCD                   | `https://<LB_IP>:2379`                     | -                   |

Direct access (internal IPs):
- Master1: See `terraform output master1_private_ip`
- Master2: See `terraform output master2_private_ip`
- Worker1: See `terraform output worker1_private_ip`
- Worker2: See `terraform output worker2_private_ip`

## ğŸ“ Project Structure

```
kubernetes_project/
â”œâ”€â”€ terraform/                    # Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf                  # Main infrastructure
â”‚   â”œâ”€â”€ variables.tf            # Input variables
â”‚   â”œâ”€â”€ outputs.tf              # Output values
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ vpc/                # Network configuration
â”‚       â”œâ”€â”€ security_groups/    # Security rules
â”‚       â””â”€â”€ ec2_cluster/        # EC2 instances
â”œâ”€â”€ ansible/                     # Configuration Management
â”‚   â”œâ”€â”€ ansible.cfg             # Ansible configuration
â”‚   â”œâ”€â”€ inventory.ini           # Host inventory
â”‚   â”œâ”€â”€ files/                  # Static files and manifests
â”‚   â”‚   â”œâ”€â”€ mongo.yaml          # MongoDB deployment
â”‚   â”‚   â”œâ”€â”€ mongo-express.yaml  # Mongo Express deployment
â”‚   â”‚   â”œâ”€â”€ simple-app.yaml     # Simple web app
â”‚   â”‚   â””â”€â”€ dashboard-admin-user.yaml # Dashboard admin user
â”‚   â”œâ”€â”€ playbooks/              # Deployment playbooks
â”‚   â”‚   â”œâ”€â”€ deploy-all.yml      # Full deployment
â”‚   â”‚   â”œâ”€â”€ etcd-playbook.yml   # ETCD cluster
â”‚   â”‚   â”œâ”€â”€ haproxy-playbook.yml # Load balancer
â”‚   â”‚   â”œâ”€â”€ k3s-masters.yml     # K3s masters
â”‚   â”‚   â”œâ”€â”€ k3s-workers.yml     # K3s workers
â”‚   â”‚   â”œâ”€â”€ k8s-dashboard.yml   # Dashboard
â”‚   â”‚   â”œâ”€â”€ deploy-app.yml      # Simple app
â”‚   â”‚   â””â”€â”€ deploy-mongo.yml    # MongoDB + Express
â”‚   â””â”€â”€ roles/                  # Ansible roles
â”‚       â”œâ”€â”€ etcd/               # ETCD cluster role
â”‚       â”œâ”€â”€ haproxy/            # HAProxy load balancer
â”‚       â”œâ”€â”€ k3s_common/         # K3s common setup
â”‚       â”œâ”€â”€ k3s_server/        # K3s master setup
â”‚       â””â”€â”€ k3s_agent/          # K3s worker setup
â”œâ”€â”€ HANDOFF_SUMMARY.txt         # Project status for handoff
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ usefullcommand.txt          # Useful commands reference
```

## ğŸ”§ Configuration Details

### ETCD Configuration
- **Version**: 3.5.9
- **TLS**: Enabled with custom CA
- **Cluster**: 2 nodes (etcd1, etcd2)
- **Data Directory**: `/var/lib/etcd`
- **Client Port**: 2379
- **Peer Port**: 2380

### K3s Configuration
- **Version**: v1.33.4+k3s1
- **Datastore**: External ETCD via HAProxy
- **TLS SANs**: Includes HAProxy IP
- **Masters**: 2 nodes with embedded control plane
- **Workers**: 2 nodes

### HAProxy Configuration
- **Stats Port**: 8399
- **Frontends**: etcd, kube-api, dashboard, simple-app, mongo-express
- **Backends**: Round-robin to respective nodes
- **Health Checks**: Enabled for all services

### Security Groups
- **Bastion SG**: SSH access from anywhere
- **Private EC2 SG**: SSH from bastion, internal VPC traffic
- **Load Balancer SG**: HTTP/HTTPS, specific ports for services

## ğŸ› ï¸ Troubleshooting

### Common Issues
1. **ETCD Permission Denied**: Check CA key permissions on controller
2. **K3s Service Not Starting**: Verify ETCD connectivity and TLS certs
3. **HAProxy Backend Down**: Check NodePort services on masters
4. **Dashboard Token Expired**: Re-run dashboard playbook

### Useful Commands
```bash
# Check cluster status
kubectl get nodes
kubectl get pods --all-namespaces

# Check ETCD health
etcdctl --endpoints=https://<LB_IP>:2379 --cacert=/etc/ssl/etcd/ca.crt --cert=/etc/ssl/etcd/server.crt --key=/etc/ssl/etcd/server.key endpoint health

# HAProxy stats
curl http://<LB_IP>:8399/stats_secure

# View logs
journalctl -u etcd -f
journalctl -u k3s -f
```

## ğŸ”’ Security Considerations

- **TLS Everywhere**: ETCD, K8s API, and Dashboard use TLS
- **Certificate Management**: Custom CA with proper SANs
- **Network Security**: Security groups restrict access
- **Bastion Host**: SSH access through bastion only
- **Passwords**: Change default passwords in production

## ğŸ“ˆ Next Steps

1. **Storage Implementation**: Deploy iSCSI + OCFS2 shared storage
2. **Monitoring Stack**: Prometheus + Grafana
3. **Backup Strategy**: ETCD snapshots and cluster backups
4. **CI/CD Pipeline**: GitOps with ArgoCD
5. **Security Hardening**: Network policies, RBAC, secrets management

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**Last Updated**: September 13, 2025
**Status**: Fully Deployed and Operational
â””â”€â”€ README.md                   # This file
```

## ğŸ”§ Components

### Infrastructure (Terraform)
- **VPC**: 10.0.0.0/16 with public/private subnets across 3 AZs
- **EC2 Instances**:
  - Bastion host (public)
  - Load balancer (public)
  - 2x ETCD nodes (private)
  - 2x Master nodes (private)
  - 2x Worker nodes (private)
  - Storage node (private)

### HAProxy Load Balancer
- **Ports**: 2379 (ETCD), 6443 (Kubernetes API), 8399 (Stats)
- **Features**: Health checks, round-robin load balancing
- **Stats Access**: http://lb-ip:8399/stats_secure (admin/omar123)

### ETCD Cluster âœ… OPERATIONAL
- **Version**: 3.5.9
- **Nodes**: 2 nodes (etcd1: 10.0.13.108, etcd2: 10.0.23.108)
- **Security**: TLS certificates for all communication
- **Status**: Both nodes active and healthy
- **Access**: Available through HAProxy at 18.209.164.58:2379
- **Health Checks**: Passing - cluster formation successful

## ğŸ”’ Security Features

- **Network Security**: Security groups with minimal required access
- **SSH Access**: Only through bastion host
- **TLS Encryption**: ETCD with certificate-based authentication
- **Load Balancer Access**: Restricted to necessary ports

## ğŸ“ˆ Monitoring & Access

### HAProxy Statistics
- **URL**: http://[load-balancer-ip]:8399/stats_secure
- **Credentials**: admin / omar123
- **Features**: Real-time connection stats, backend health

### SSH Access Pattern
```bash
# Connect to bastion
ssh -i ~/.ssh/id_rsa ec2-user@bastion-ip

# From bastion, connect to private instances
ssh -i ~/.ssh/id_rsa ec2-user@private-instance-ip
```

## ğŸš§ Next Steps

1. **âœ… ETCD Cluster - COMPLETED**
   - Both ETCD nodes deployed and operational
   - TLS certificates configured
   - HAProxy load balancer working
   - Cluster health verified

2. **ğŸ”„ Implement Kubernetes Masters - NEXT PRIORITY**
   - Create Ansible role for Kubernetes control plane
   - Configure master nodes to use ETCD cluster
   - Set up Kubernetes API server with ETCD endpoint: `https://18.209.164.58:2379`

3. **Deploy Worker Nodes**
   - Configure worker nodes
   - Join Kubernetes cluster

4. **Set up Shared Storage**
   - iSCSI target on storage node
   - OCFS2 filesystem
   - Mount on all nodes

5. **Configure Networking**
   - Calico/Cilium CNI
   - Network policies

6. **Deploy Monitoring**
   - Prometheus + Grafana
   - AlertManager

## ğŸ› Troubleshooting

### Common Issues
- **SSH Connection Failed**: Check security groups and key permissions
- **Ansible Connection Error**: Verify inventory.ini IP addresses
- **HAProxy Not Accessible**: Check security group rules for port 8399

### Logs
```bash
# HAProxy logs
sudo journalctl -u haproxy -f

# ETCD logs
sudo journalctl -u etcd -f

# System logs
sudo journalctl -xe
```

## ğŸ“š Resources

- [Medium Guide: Production Kubernetes Cluster](https://medium.com/@augustineozor/deploying-a-production-ready-kubernetes-cluster-in-2023-2024-a-step-by-step-guide-part-1-8f6b2e8c1c1a)
- [ETCD Documentation](https://etcd.io/docs/)
- [HAProxy Documentation](https://www.haproxy.org/)
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest)

## ğŸ¤ Contributing

This project follows the implementation from the Medium guide. For questions or issues, please check the troubleshooting section or refer to the original guide.

---

**Status**: Infrastructure âœ… | HAProxy âœ… | ETCD âœ… | Kubernetes Masters ğŸ”„ Next Phase
